#!/usr/bin/env python3
"""
ðŸ“ˆ IRONFORGE Weeklyâ†’Daily Liquidity Sweep Cascade Lattice
=========================================================

Macro-Level Cascade Pattern Discovery Framework
Maps higher timeframe (Weeklyâ†’Daily) liquidity sweep cascade patterns across session networks.

Core Architecture:
- Weekly liquidity formation â†’ Daily cascade propagation
- Cross-session sweep relationship mapping  
- HTF influence transmission pattern analysis
- Macro-to-micro cascade timing validation

Discovery Focus:
1. Weekly HTF Liquidity Formation Events
2. Daily Session Cascade Propagation Mapping
3. Cross-Session Sweep Relationship Analysis
4. HTFâ†’Session Transmission Timing Patterns
5. Macro Cascade Predictive Framework

Archaeological Significance:
- Maps how Weekly events cascade into Daily session structure
- Identifies macro liquidity sweep patterns that influence session behavior
- Tests HTFâ†’LTF transmission timing and causality relationships
- Provides macro-level complement to micro-level FVG redelivery analysis
"""

import json
import logging
from datetime import datetime, time, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import sys

# Add IRONFORGE to path
sys.path.append(str(Path(__file__).parent.parent))

from config import get_config

logger = logging.getLogger(__name__)

class WeeklyDailySweepCascadeLattice:
    """
    Weeklyâ†’Daily Liquidity Sweep Cascade Lattice Builder
    
    Maps macro-level cascade patterns where Weekly HTF events influence Daily session structure
    through liquidity sweep transmission mechanisms.
    """
    
    def __init__(self):
        """Initialize Weeklyâ†’Daily sweep cascade lattice builder"""
        self.config = get_config()
        self.enhanced_sessions_path = Path(self.config.get_enhanced_data_path())
        self.discoveries_path = Path(self.config.get_discoveries_path())
        
        # Cascade analysis parameters
        self.htf_timeframes = ['Weekly', 'Daily', '4H', '1H']
        self.session_timeframes = ['NY_AM', 'LONDON', 'ASIA', 'NY_PM']
        self.cascade_detection_window = 7  # Days to look for cascade propagation
        
        # Liquidity sweep identification parameters
        self.sweep_types = [
            'liquidity_sweep', 'stop_hunt', 'liquidity_grab',
            'sweep_higher', 'sweep_lower', 'liquidity_raid'
        ]
        
        # Cascade timing analysis parameters
        self.timing_precision_thresholds = {
            'immediate': 4,  # Hours
            'short_term': 24,  # Hours
            'medium_term': 72,  # Hours
            'long_term': 168  # Hours (1 week)
        }
        
    def build_weekly_daily_cascade_lattice(self, sessions_limit: Optional[int] = None) -> Dict[str, Any]:
        """
        Build comprehensive Weeklyâ†’Daily liquidity sweep cascade lattice
        
        Cascade Mapping Focus:
        - Weekly HTF liquidity formation events
        - Daily session cascade propagation patterns
        - Cross-session sweep relationship mapping
        - HTFâ†’Session transmission timing analysis
        
        Returns:
            Dict containing complete Weeklyâ†’Daily cascade lattice analysis
        """
        logger.info("Building Weeklyâ†’Daily Liquidity Sweep Cascade Lattice...")
        
        try:
            # Initialize lattice structure
            lattice = {
                'lattice_type': 'weekly_daily_sweep_cascade',
                'lattice_metadata': {
                    'build_timestamp': datetime.now().isoformat(),
                    'focus': 'weekly_to_daily_liquidity_sweep_cascades',
                    'htf_timeframes': self.htf_timeframes,
                    'session_timeframes': self.session_timeframes,
                    'cascade_detection_window_days': self.cascade_detection_window
                }
            }
            
            # Load enhanced sessions
            enhanced_sessions = self._load_enhanced_sessions(sessions_limit)
            lattice['lattice_metadata']['sessions_analyzed'] = len(enhanced_sessions)
            
            if not enhanced_sessions:
                lattice['error'] = "No enhanced sessions found for cascade analysis"
                return lattice
            
            # Extract Weekly HTF liquidity formation events
            weekly_liquidity_events = self._extract_weekly_liquidity_events(enhanced_sessions)
            lattice['weekly_liquidity_events'] = weekly_liquidity_events
            
            # Extract Daily session liquidity sweep events
            daily_sweep_events = self._extract_daily_sweep_events(enhanced_sessions)
            lattice['daily_sweep_events'] = daily_sweep_events
            
            # Map Weeklyâ†’Daily cascade relationships
            cascade_relationships = self._map_weekly_daily_cascade_relationships(
                weekly_liquidity_events, daily_sweep_events, enhanced_sessions
            )
            lattice['cascade_relationships'] = cascade_relationships
            
            # Analyze cascade timing patterns
            timing_analysis = self._analyze_cascade_timing_patterns(cascade_relationships)
            lattice['cascade_timing_analysis'] = timing_analysis
            
            # Cross-session sweep propagation analysis
            sweep_propagation = self._analyze_cross_session_sweep_propagation(daily_sweep_events)
            lattice['sweep_propagation_analysis'] = sweep_propagation
            
            # HTF influence transmission analysis
            htf_transmission = self._analyze_htf_influence_transmission(
                weekly_liquidity_events, enhanced_sessions
            )
            lattice['htf_transmission_analysis'] = htf_transmission
            
            # Cascade network topology analysis
            network_topology = self._analyze_cascade_network_topology(cascade_relationships)
            lattice['cascade_network_topology'] = network_topology
            
            # Macro cascade predictive patterns
            predictive_patterns = self._identify_macro_cascade_predictive_patterns(lattice)
            lattice['predictive_patterns'] = predictive_patterns
            
            # Discovery insights and recommendations
            discovery_insights = self._generate_cascade_discovery_insights(lattice)
            lattice['discovery_insights'] = discovery_insights
            
            # Save lattice to discoveries
            self._save_weekly_daily_cascade_lattice(lattice)
            
            logger.info(f"Weeklyâ†’Daily Cascade Lattice built: {len(cascade_relationships)} cascade relationships mapped")
            return lattice
            
        except Exception as e:
            logger.error(f"Failed to build Weeklyâ†’Daily cascade lattice: {e}")
            return {
                'lattice_type': 'weekly_daily_sweep_cascade',
                'error': str(e),
                'lattice_metadata': {
                    'build_timestamp': datetime.now().isoformat(),
                    'build_failed': True
                }
            }
    
    def _load_enhanced_sessions(self, sessions_limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Load enhanced session data for cascade analysis"""
        enhanced_sessions = []
        session_files = list(self.enhanced_sessions_path.glob("enhanced_rel_*.json"))
        
        # Sort by modification time (most recent first) for chronological analysis
        session_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        if sessions_limit:
            session_files = session_files[:sessions_limit]
        
        for session_file in session_files:
            try:
                with open(session_file, 'r') as f:
                    session_data = json.load(f)
                    # Add chronological information for cascade analysis
                    session_data['file_timestamp'] = session_file.stat().st_mtime
                    enhanced_sessions.append(session_data)
            except Exception as e:
                logger.warning(f"Failed to load session {session_file}: {e}")
                continue
        
        # Sort by session date for chronological cascade analysis
        enhanced_sessions.sort(key=lambda x: x.get('session_date', ''), reverse=True)
        
        logger.info(f"Loaded {len(enhanced_sessions)} enhanced sessions for cascade analysis")
        return enhanced_sessions
    
    def _extract_weekly_liquidity_events(self, enhanced_sessions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Extract Weekly HTF liquidity formation events that could cascade to Daily sessions
        
        Focus Areas:
        1. Weekly liquidity pool formations
        2. Weekly range extremes and liquidity concentrations
        3. Weekly structural breaks and regime changes
        4. Cross-week liquidity persistence patterns
        """
        weekly_events = []
        
        for session in enhanced_sessions:
            session_name = session.get('session_name', 'unknown')
            session_date = session.get('session_date', 'unknown')
            
            # Extract weekly-level liquidity events from session data
            weekly_session_events = self._extract_weekly_events_from_session(session)
            
            for event in weekly_session_events:
                weekly_event = {
                    'source_session': session_name,
                    'session_date': session_date,
                    'event_timestamp': event.get('timestamp'),
                    'liquidity_type': self._classify_weekly_liquidity_type(event),
                    'liquidity_level': self._extract_liquidity_level(event),
                    'formation_context': self._get_weekly_formation_context(event, session),
                    'potential_cascade_targets': self._identify_potential_cascade_targets(event),
                    'weekly_significance': self._assess_weekly_significance(event, session),
                    'raw_event': event
                }
                weekly_events.append(weekly_event)
        
        logger.info(f"Extracted {len(weekly_events)} Weekly HTF liquidity events")
        return weekly_events
    
    def _extract_weekly_events_from_session(self, session: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract events that represent Weekly timeframe liquidity formations"""
        weekly_events = []
        
        # Look for Weekly-level events in different sources
        event_sources = [
            'semantic_events',
            'session_liquidity_events', 
            'structural_events',
            'price_movements'
        ]
        
        for source in event_sources:
            events = session.get(source, [])
            for event in events:
                if self._is_weekly_timeframe_event(event):
                    weekly_events.append(event)
        
        return weekly_events
    
    def _is_weekly_timeframe_event(self, event: Dict[str, Any]) -> bool:
        """Determine if event represents Weekly timeframe liquidity formation"""
        event_text = str(event).lower()
        
        # Check for Weekly timeframe indicators
        weekly_indicators = [
            'weekly', 'week', 'htf', 'higher timeframe',
            'weekly high', 'weekly low', 'weekly range',
            'weekly liquidity', 'weekly structural',
            'multi-day', 'cross-session'
        ]
        
        return any(indicator in event_text for indicator in weekly_indicators)
    
    def _classify_weekly_liquidity_type(self, event: Dict[str, Any]) -> str:
        """Classify type of Weekly liquidity formation"""
        event_text = str(event).lower()
        
        if 'pool' in event_text or 'accumulation' in event_text:
            return 'liquidity_pool'
        elif 'sweep' in event_text or 'hunt' in event_text:
            return 'liquidity_sweep'
        elif 'high' in event_text and ('weekly' in event_text or 'htf' in event_text):
            return 'weekly_high_liquidity'
        elif 'low' in event_text and ('weekly' in event_text or 'htf' in event_text):
            return 'weekly_low_liquidity'
        elif 'break' in event_text or 'structural' in event_text:
            return 'structural_break'
        else:
            return 'unknown_weekly_liquidity'
    
    def _extract_liquidity_level(self, event: Dict[str, Any]) -> float:
        """Extract price level of liquidity formation"""
        price = event.get('price_level', event.get('price', 0))
        try:
            return float(price) if price is not None else 0.0
        except (ValueError, TypeError):
            return 0.0
    
    def _get_weekly_formation_context(self, event: Dict[str, Any], session: Dict[str, Any]) -> Dict[str, Any]:
        """Get context for Weekly liquidity formation"""
        return {
            'session_timeframe': self._get_session_timeframe(session.get('session_name', '')),
            'formation_significance': self._assess_formation_significance(event),
            'structural_context': self._get_structural_context(event, session),
            'timing_context': self._get_timing_context(event, session)
        }
    
    def _identify_potential_cascade_targets(self, event: Dict[str, Any]) -> List[str]:
        """Identify potential Daily session targets for Weekly liquidity cascade"""
        liquidity_type = self._classify_weekly_liquidity_type(event)
        
        # Map Weekly liquidity types to likely Daily session cascade targets
        cascade_mapping = {
            'liquidity_pool': ['NY_AM', 'LONDON', 'NY_PM'],
            'liquidity_sweep': ['NY_PM', 'ASIA'],
            'weekly_high_liquidity': ['NY_AM', 'LONDON'],
            'weekly_low_liquidity': ['NY_PM', 'ASIA'],
            'structural_break': ['NY_AM', 'NY_PM']
        }
        
        return cascade_mapping.get(liquidity_type, ['NY_AM', 'NY_PM'])
    
    def _assess_weekly_significance(self, event: Dict[str, Any], session: Dict[str, Any]) -> float:
        """Assess significance of Weekly liquidity formation for cascade potential"""
        significance_factors = []
        
        # Price level significance
        liquidity_level = self._extract_liquidity_level(event)
        if liquidity_level > 0:
            session_range = self._calculate_session_range(session)
            if session_range['range'] > 0:
                range_position = (liquidity_level - session_range['low']) / session_range['range']
                # Higher significance for extremes (near 0 or 1)
                position_significance = 1 - abs(range_position - 0.5) * 2
                significance_factors.append(position_significance)
        
        # Event type significance
        liquidity_type = self._classify_weekly_liquidity_type(event)
        type_significance = {
            'liquidity_pool': 0.8,
            'liquidity_sweep': 0.9,
            'weekly_high_liquidity': 0.7,
            'weekly_low_liquidity': 0.7,
            'structural_break': 0.95,
            'unknown_weekly_liquidity': 0.3
        }
        significance_factors.append(type_significance.get(liquidity_type, 0.5))
        
        # Return average significance
        return sum(significance_factors) / len(significance_factors) if significance_factors else 0.5
    
    def _extract_daily_sweep_events(self, enhanced_sessions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Extract Daily session liquidity sweep events that could be cascade targets
        
        Focus Areas:
        1. Daily session liquidity sweeps
        2. Intraday sweep patterns
        3. Session-to-session sweep relationships
        4. Daily range extreme tests and sweeps
        """
        daily_sweep_events = []
        
        for session in enhanced_sessions:
            session_name = session.get('session_name', 'unknown')
            session_date = session.get('session_date', 'unknown')
            
            # Extract daily sweep events from session
            session_sweeps = self._extract_session_sweep_events(session)
            
            for sweep in session_sweeps:
                daily_sweep = {
                    'session_name': session_name,
                    'session_date': session_date,
                    'session_timeframe': self._get_session_timeframe(session_name),
                    'sweep_timestamp': sweep.get('timestamp'),
                    'sweep_type': self._classify_daily_sweep_type(sweep),
                    'sweep_target_level': self._extract_sweep_target_level(sweep),
                    'sweep_direction': self._determine_sweep_direction(sweep),
                    'session_context': self._get_daily_sweep_session_context(sweep, session),
                    'cascade_receptivity': self._assess_cascade_receptivity(sweep, session),
                    'raw_event': sweep
                }
                daily_sweep_events.append(daily_sweep)
        
        logger.info(f"Extracted {len(daily_sweep_events)} Daily session sweep events")
        return daily_sweep_events
    
    def _extract_session_sweep_events(self, session: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract liquidity sweep events from session data"""
        sweep_events = []
        
        event_sources = [
            'semantic_events',
            'session_liquidity_events',
            'price_movements'
        ]
        
        for source in event_sources:
            events = session.get(source, [])
            for event in events:
                if self._is_daily_sweep_event(event):
                    sweep_events.append(event)
        
        return sweep_events
    
    def _is_daily_sweep_event(self, event: Dict[str, Any]) -> bool:
        """Determine if event represents Daily session liquidity sweep"""
        event_text = str(event).lower()
        
        return any(sweep_type in event_text for sweep_type in self.sweep_types)
    
    def _classify_daily_sweep_type(self, event: Dict[str, Any]) -> str:
        """Classify type of Daily session sweep"""
        event_text = str(event).lower()
        
        for sweep_type in self.sweep_types:
            if sweep_type in event_text:
                return sweep_type
        
        return 'unknown_sweep'
    
    def _extract_sweep_target_level(self, event: Dict[str, Any]) -> float:
        """Extract target price level of liquidity sweep"""
        return self._extract_liquidity_level(event)
    
    def _determine_sweep_direction(self, event: Dict[str, Any]) -> str:
        """Determine direction of liquidity sweep"""
        event_text = str(event).lower()
        
        if 'higher' in event_text or 'up' in event_text or 'above' in event_text:
            return 'sweep_higher'
        elif 'lower' in event_text or 'down' in event_text or 'below' in event_text:
            return 'sweep_lower'
        else:
            return 'unknown_direction'
    
    def _get_daily_sweep_session_context(self, event: Dict[str, Any], session: Dict[str, Any]) -> Dict[str, Any]:
        """Get session context for Daily sweep event"""
        return {
            'sweep_timing_in_session': self._calculate_sweep_timing_in_session(event, session),
            'session_phase': self._determine_session_phase_for_sweep(event, session),
            'relative_session_position': self._calculate_relative_session_position(event, session)
        }
    
    def _assess_cascade_receptivity(self, event: Dict[str, Any], session: Dict[str, Any]) -> float:
        """Assess how receptive this sweep is to Weekly HTF cascade influence"""
        receptivity_factors = []
        
        # Timing factor - sweeps early in session more receptive to HTF influence
        timing = self._calculate_sweep_timing_in_session(event, session)
        timing_receptivity = 1 - timing  # Earlier = higher receptivity
        receptivity_factors.append(timing_receptivity)
        
        # Sweep type factor
        sweep_type = self._classify_daily_sweep_type(event)
        type_receptivity = {
            'liquidity_sweep': 0.9,
            'stop_hunt': 0.8,
            'liquidity_grab': 0.7,
            'sweep_higher': 0.6,
            'sweep_lower': 0.6,
            'liquidity_raid': 0.95,
            'unknown_sweep': 0.4
        }
        receptivity_factors.append(type_receptivity.get(sweep_type, 0.5))
        
        return sum(receptivity_factors) / len(receptivity_factors)
    
    def _map_weekly_daily_cascade_relationships(self, weekly_events: List[Dict[str, Any]], 
                                               daily_sweeps: List[Dict[str, Any]], 
                                               sessions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Map Weekly HTF liquidity events to Daily session sweep cascades
        
        Cascade Identification Logic:
        1. Temporal proximity (Weekly event â†’ Daily sweep within detection window)
        2. Price level correlation (Weekly liquidity level â‰ˆ Daily sweep target)
        3. Cascade target matching (Weekly targets match Daily session timeframe)
        4. Cascade timing analysis (HTFâ†’LTF transmission timing patterns)
        """
        cascade_relationships = []
        
        for weekly_event in weekly_events:
            weekly_timestamp = weekly_event['event_timestamp']
            weekly_level = weekly_event['liquidity_level']
            weekly_targets = weekly_event['potential_cascade_targets']
            
            # Find Daily sweeps that could be cascades from this Weekly event
            for daily_sweep in daily_sweeps:
                # Check temporal proximity
                if self._is_within_cascade_window(weekly_timestamp, daily_sweep['sweep_timestamp']):
                    
                    # Check price level correlation
                    sweep_level = daily_sweep['sweep_target_level']
                    if self._is_price_level_correlated(weekly_level, sweep_level):
                        
                        # Check cascade target matching
                        session_timeframe = daily_sweep['session_timeframe']
                        if session_timeframe in weekly_targets or not weekly_targets:
                            
                            # Calculate cascade relationship strength
                            relationship_strength = self._calculate_cascade_relationship_strength(
                                weekly_event, daily_sweep
                            )
                            
                            if relationship_strength > 0.5:  # Strong relationship threshold
                                cascade = {
                                    'weekly_event': weekly_event,
                                    'daily_sweep': daily_sweep,
                                    'cascade_strength': relationship_strength,
                                    'cascade_timing': self._calculate_cascade_timing(
                                        weekly_timestamp, daily_sweep['sweep_timestamp']
                                    ),
                                    'price_correlation': self._calculate_price_correlation(
                                        weekly_level, sweep_level
                                    ),
                                    'cascade_type': self._classify_cascade_type(weekly_event, daily_sweep),
                                    'transmission_analysis': self._analyze_transmission_mechanism(
                                        weekly_event, daily_sweep
                                    )
                                }
                                cascade_relationships.append(cascade)
        
        logger.info(f"Mapped {len(cascade_relationships)} Weeklyâ†’Daily cascade relationships")
        return cascade_relationships
    
    def _is_within_cascade_window(self, weekly_timestamp: str, daily_timestamp: str) -> bool:
        """Check if Daily event is within cascade detection window of Weekly event"""
        try:
            # Simple time proximity check (would need proper datetime parsing in production)
            return True  # Simplified for this implementation
        except:
            return False
    
    def _is_price_level_correlated(self, weekly_level: float, daily_level: float) -> bool:
        """Check if Weekly and Daily price levels are correlated for cascade"""
        if weekly_level == 0 or daily_level == 0:
            return False
        
        # Allow for reasonable price difference (within 1% or 100 points)
        price_diff = abs(weekly_level - daily_level)
        percentage_diff = price_diff / weekly_level if weekly_level > 0 else 1
        
        return price_diff < 100 or percentage_diff < 0.01
    
    def _calculate_cascade_relationship_strength(self, weekly_event: Dict[str, Any], 
                                               daily_sweep: Dict[str, Any]) -> float:
        """Calculate strength of Weeklyâ†’Daily cascade relationship"""
        strength_factors = []
        
        # Weekly event significance
        weekly_significance = weekly_event['weekly_significance']
        strength_factors.append(weekly_significance)
        
        # Daily sweep cascade receptivity
        cascade_receptivity = daily_sweep['cascade_receptivity']
        strength_factors.append(cascade_receptivity)
        
        # Price level correlation
        weekly_level = weekly_event['liquidity_level']
        daily_level = daily_sweep['sweep_target_level']
        if weekly_level > 0 and daily_level > 0:
            price_correlation = 1 - (abs(weekly_level - daily_level) / max(weekly_level, daily_level))
            strength_factors.append(max(0, price_correlation))
        
        # Target matching factor
        session_timeframe = daily_sweep['session_timeframe']
        potential_targets = weekly_event['potential_cascade_targets']
        if session_timeframe in potential_targets:
            strength_factors.append(0.8)
        else:
            strength_factors.append(0.4)
        
        return sum(strength_factors) / len(strength_factors)
    
    def _calculate_cascade_timing(self, weekly_timestamp: str, daily_timestamp: str) -> Dict[str, Any]:
        """Calculate cascade timing characteristics"""
        # Simplified timing analysis
        return {
            'timing_category': 'short_term',  # Would calculate actual timing
            'transmission_delay_hours': 24,   # Placeholder
            'timing_precision': 'medium'
        }
    
    def _calculate_price_correlation(self, weekly_level: float, daily_level: float) -> float:
        """Calculate price level correlation between Weekly and Daily events"""
        if weekly_level == 0 or daily_level == 0:
            return 0.0
        
        price_diff = abs(weekly_level - daily_level)
        max_price = max(weekly_level, daily_level)
        
        return max(0, 1 - (price_diff / max_price))
    
    def _classify_cascade_type(self, weekly_event: Dict[str, Any], daily_sweep: Dict[str, Any]) -> str:
        """Classify type of Weeklyâ†’Daily cascade"""
        weekly_type = weekly_event['liquidity_type']
        daily_type = daily_sweep['sweep_type']
        
        # Map Weeklyâ†’Daily cascade patterns
        if weekly_type == 'liquidity_pool' and 'sweep' in daily_type:
            return 'pool_to_sweep_cascade'
        elif weekly_type == 'weekly_high_liquidity' and 'higher' in daily_type:
            return 'weekly_high_cascade'
        elif weekly_type == 'weekly_low_liquidity' and 'lower' in daily_type:
            return 'weekly_low_cascade'
        elif weekly_type == 'structural_break':
            return 'structural_break_cascade'
        else:
            return 'unknown_cascade_type'
    
    def _analyze_transmission_mechanism(self, weekly_event: Dict[str, Any], 
                                      daily_sweep: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze HTFâ†’LTF transmission mechanism"""
        return {
            'transmission_type': 'direct_price_cascade',  # Simplified
            'mechanism_strength': 0.7,
            'intermediary_timeframes': ['Daily', '4H'],
            'transmission_efficiency': 0.8
        }
    
    # Additional helper methods for completeness
    def _get_session_timeframe(self, session_name: str) -> str:
        """Extract timeframe from session name"""
        if 'NY_AM' in session_name:
            return 'NY_AM'
        elif 'NY_PM' in session_name:
            return 'NY_PM'
        elif 'LONDON' in session_name:
            return 'LONDON'
        elif 'ASIA' in session_name:
            return 'ASIA'
        else:
            return 'unknown'
    
    def _calculate_session_range(self, session: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate session high/low range"""
        if 'session_high' in session and 'session_low' in session:
            return {
                'high': session['session_high'],
                'low': session['session_low'],
                'range': session['session_high'] - session['session_low']
            }
        else:
            return {'high': 0, 'low': 0, 'range': 0}
    
    def _assess_formation_significance(self, event: Dict[str, Any]) -> float:
        """Assess significance of formation event"""
        return 0.7  # Placeholder
    
    def _get_structural_context(self, event: Dict[str, Any], session: Dict[str, Any]) -> str:
        """Get structural context of event"""
        return 'session_formation'  # Placeholder
    
    def _get_timing_context(self, event: Dict[str, Any], session: Dict[str, Any]) -> str:
        """Get timing context of event"""
        return 'mid_session'  # Placeholder
    
    def _calculate_sweep_timing_in_session(self, event: Dict[str, Any], session: Dict[str, Any]) -> float:
        """Calculate timing of sweep within session (0-1)"""
        return 0.5  # Placeholder
    
    def _determine_session_phase_for_sweep(self, event: Dict[str, Any], session: Dict[str, Any]) -> str:
        """Determine session phase for sweep"""
        return 'middle'  # Placeholder
    
    def _calculate_relative_session_position(self, event: Dict[str, Any], session: Dict[str, Any]) -> float:
        """Calculate relative position in session"""
        return 0.5  # Placeholder
    
    def _analyze_cascade_timing_patterns(self, cascade_relationships: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze timing patterns in Weeklyâ†’Daily cascades"""
        timing_analysis = {
            'total_cascades_analyzed': len(cascade_relationships),
            'timing_distribution': {},
            'average_transmission_delay': 0,
            'timing_precision_analysis': {},
            'cascade_velocity_patterns': []
        }
        
        if not cascade_relationships:
            return timing_analysis
        
        # Analyze timing distribution
        timing_categories = {}
        transmission_delays = []
        
        for cascade in cascade_relationships:
            timing = cascade.get('cascade_timing', {})
            timing_category = timing.get('timing_category', 'unknown')
            delay_hours = timing.get('transmission_delay_hours', 0)
            
            timing_categories[timing_category] = timing_categories.get(timing_category, 0) + 1
            if delay_hours > 0:
                transmission_delays.append(delay_hours)
        
        timing_analysis['timing_distribution'] = timing_categories
        
        if transmission_delays:
            timing_analysis['average_transmission_delay'] = sum(transmission_delays) / len(transmission_delays)
        
        # Analyze timing precision
        precision_analysis = self._analyze_timing_precision(cascade_relationships)
        timing_analysis['timing_precision_analysis'] = precision_analysis
        
        return timing_analysis
    
    def _analyze_timing_precision(self, cascade_relationships: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze timing precision of cascades"""
        precision_counts = {}
        
        for cascade in cascade_relationships:
            timing = cascade.get('cascade_timing', {})
            precision = timing.get('timing_precision', 'unknown')
            precision_counts[precision] = precision_counts.get(precision, 0) + 1
        
        return precision_counts
    
    def _analyze_cross_session_sweep_propagation(self, daily_sweep_events: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze how sweeps propagate across different session timeframes"""
        propagation_analysis = {
            'session_sweep_distribution': {},
            'cross_session_propagation_chains': [],
            'session_to_session_transmission_patterns': {},
            'sweep_clustering_analysis': {}
        }
        
        # Distribution by session timeframe
        session_distribution = {}
        for sweep in daily_sweep_events:
            session_tf = sweep['session_timeframe']
            session_distribution[session_tf] = session_distribution.get(session_tf, 0) + 1
        
        propagation_analysis['session_sweep_distribution'] = session_distribution
        
        # Find propagation chains across sessions
        propagation_chains = self._find_cross_session_propagation_chains(daily_sweep_events)
        propagation_analysis['cross_session_propagation_chains'] = propagation_chains
        
        return propagation_analysis
    
    def _find_cross_session_propagation_chains(self, daily_sweep_events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Find chains of sweeps that propagate across sessions"""
        propagation_chains = []
        
        # Group sweeps by similar price levels
        price_groups = {}
        for sweep in daily_sweep_events:
            price = sweep['sweep_target_level']
            price_bucket = round(price / 50) * 50  # Group into 50-point buckets
            
            if price_bucket not in price_groups:
                price_groups[price_bucket] = []
            price_groups[price_bucket].append(sweep)
        
        # Find chains with multiple sessions
        for price_level, sweeps in price_groups.items():
            if len(sweeps) >= 2:  # At least 2 sweeps at similar price
                sessions_involved = set(sweep['session_timeframe'] for sweep in sweeps)
                if len(sessions_involved) >= 2:  # Across multiple sessions
                    chain = {
                        'price_level': price_level,
                        'sweep_count': len(sweeps),
                        'sessions_involved': list(sessions_involved),
                        'propagation_span': len(sessions_involved),
                        'chain_strength': len(sweeps) * len(sessions_involved)
                    }
                    propagation_chains.append(chain)
        
        return sorted(propagation_chains, key=lambda x: x['chain_strength'], reverse=True)
    
    def _analyze_htf_influence_transmission(self, weekly_events: List[Dict[str, Any]], 
                                          sessions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze how Weekly HTF events influence session structure"""
        transmission_analysis = {
            'total_weekly_events': len(weekly_events),
            'transmission_efficiency_by_type': {},
            'session_receptivity_analysis': {},
            'htf_influence_patterns': []
        }
        
        # Analyze transmission efficiency by Weekly event type
        type_efficiency = {}
        for event in weekly_events:
            event_type = event['liquidity_type']
            weekly_significance = event['weekly_significance']
            
            if event_type not in type_efficiency:
                type_efficiency[event_type] = {'total_significance': 0, 'count': 0}
            
            type_efficiency[event_type]['total_significance'] += weekly_significance
            type_efficiency[event_type]['count'] += 1
        
        # Calculate average efficiency
        for event_type, data in type_efficiency.items():
            avg_efficiency = data['total_significance'] / data['count'] if data['count'] > 0 else 0
            transmission_analysis['transmission_efficiency_by_type'][event_type] = {
                'average_efficiency': avg_efficiency,
                'event_count': data['count']
            }
        
        return transmission_analysis
    
    def _analyze_cascade_network_topology(self, cascade_relationships: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze network topology of Weeklyâ†’Daily cascades"""
        topology = {
            'total_cascade_relationships': len(cascade_relationships),
            'network_density': 0,
            'cascade_hub_analysis': {},
            'cascade_strength_distribution': {},
            'network_efficiency': 0
        }
        
        if not cascade_relationships:
            return topology
        
        # Analyze cascade strength distribution
        strength_distribution = {}
        strength_values = []
        
        for cascade in cascade_relationships:
            strength = cascade['cascade_strength']
            strength_values.append(strength)
            
            # Categorize strength
            if strength >= 0.8:
                category = 'very_strong'
            elif strength >= 0.6:
                category = 'strong'
            elif strength >= 0.4:
                category = 'moderate'
            else:
                category = 'weak'
            
            strength_distribution[category] = strength_distribution.get(category, 0) + 1
        
        topology['cascade_strength_distribution'] = strength_distribution
        
        # Calculate network efficiency
        if strength_values:
            topology['network_efficiency'] = sum(strength_values) / len(strength_values)
        
        return topology
    
    def _identify_macro_cascade_predictive_patterns(self, lattice: Dict[str, Any]) -> Dict[str, Any]:
        """Identify predictive patterns in macro-level cascades"""
        predictive_patterns = {
            'cascade_prediction_rules': [],
            'timing_prediction_patterns': [],
            'strength_prediction_indicators': [],
            'session_targeting_patterns': []
        }
        
        cascade_relationships = lattice.get('cascade_relationships', [])
        
        # Extract prediction rules based on cascade patterns
        if cascade_relationships:
            # Rule 1: Weekly liquidity pools â†’ Daily sweeps
            pool_cascades = [c for c in cascade_relationships 
                           if c['weekly_event']['liquidity_type'] == 'liquidity_pool']
            
            if pool_cascades:
                predictive_patterns['cascade_prediction_rules'].append({
                    'rule_type': 'weekly_pool_to_daily_sweep',
                    'pattern_count': len(pool_cascades),
                    'average_strength': sum(c['cascade_strength'] for c in pool_cascades) / len(pool_cascades),
                    'prediction_confidence': 'high' if len(pool_cascades) >= 5 else 'medium'
                })
            
            # Rule 2: Structural breaks â†’ Cross-session cascades
            break_cascades = [c for c in cascade_relationships 
                            if c['weekly_event']['liquidity_type'] == 'structural_break']
            
            if break_cascades:
                predictive_patterns['cascade_prediction_rules'].append({
                    'rule_type': 'structural_break_cascade',
                    'pattern_count': len(break_cascades),
                    'average_strength': sum(c['cascade_strength'] for c in break_cascades) / len(break_cascades),
                    'prediction_confidence': 'extreme' if len(break_cascades) >= 3 else 'high'
                })
        
        return predictive_patterns
    
    def _generate_cascade_discovery_insights(self, lattice: Dict[str, Any]) -> Dict[str, Any]:
        """Generate discovery insights and recommendations for cascade analysis"""
        insights = {
            'cascade_mapping_summary': {},
            'htf_transmission_assessment': {},
            'cross_session_propagation_insights': {},
            'discovery_recommendations': []
        }
        
        # Cascade mapping summary
        cascade_relationships = lattice.get('cascade_relationships', [])
        weekly_events = lattice.get('weekly_liquidity_events', [])
        daily_sweeps = lattice.get('daily_sweep_events', [])
        
        insights['cascade_mapping_summary'] = {
            'total_weekly_events': len(weekly_events),
            'total_daily_sweeps': len(daily_sweeps),
            'mapped_cascades': len(cascade_relationships),
            'cascade_ratio': len(cascade_relationships) / len(weekly_events) if weekly_events else 0,
            'cascade_detection_success': 'HIGH' if len(cascade_relationships) > 10 else 'MODERATE' if len(cascade_relationships) > 5 else 'LOW'
        }
        
        # HTF transmission assessment
        htf_transmission = lattice.get('htf_transmission_analysis', {})
        insights['htf_transmission_assessment'] = {
            'transmission_efficiency': htf_transmission.get('transmission_efficiency_by_type', {}),
            'htf_influence_strength': 'STRONG' if len(cascade_relationships) > 15 else 'MODERATE'
        }
        
        # Cross-session propagation insights
        propagation = lattice.get('sweep_propagation_analysis', {})
        propagation_chains = propagation.get('cross_session_propagation_chains', [])
        insights['cross_session_propagation_insights'] = {
            'propagation_chains_detected': len(propagation_chains),
            'propagation_strength': 'HIGH' if len(propagation_chains) > 5 else 'MODERATE'
        }
        
        # Generate recommendations
        recommendations = []
        
        # Cascade mapping recommendations
        if len(cascade_relationships) > 10:
            recommendations.append({
                'priority': 'EXTREME',
                'type': 'macro_cascade_framework',
                'description': 'Strong Weeklyâ†’Daily cascade patterns detected',
                'action': 'Build real-time macro cascade prediction framework'
            })
        elif len(cascade_relationships) > 5:
            recommendations.append({
                'priority': 'HIGH',
                'type': 'cascade_pattern_validation',
                'description': 'Moderate cascade patterns identified',
                'action': 'Expand cascade analysis to more sessions for pattern validation'
            })
        
        # HTF transmission recommendations
        if len(weekly_events) > 20:
            recommendations.append({
                'priority': 'HIGH',
                'type': 'htf_monitoring_system',
                'description': 'Significant Weekly HTF liquidity activity detected',
                'action': 'Implement Weekly HTF monitoring system for cascade prediction'
            })
        
        # Cross-session propagation recommendations
        if len(propagation_chains) > 5:
            recommendations.append({
                'priority': 'HIGH',
                'type': 'cross_session_propagation',
                'description': 'Strong cross-session sweep propagation patterns',
                'action': 'Map complete session-to-session propagation networks'
            })
        
        insights['discovery_recommendations'] = recommendations
        
        return insights
    
    def _save_weekly_daily_cascade_lattice(self, lattice: Dict[str, Any]) -> None:
        """Save Weeklyâ†’Daily cascade lattice to discoveries directory"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"weekly_daily_cascade_lattice_{timestamp}.json"
        filepath = self.discoveries_path / filename
        
        try:
            with open(filepath, 'w') as f:
                json.dump(lattice, f, indent=2, default=str)
            logger.info(f"Weeklyâ†’Daily Cascade Lattice saved to {filepath}")
        except Exception as e:
            logger.error(f"Failed to save Weeklyâ†’Daily cascade lattice: {e}")